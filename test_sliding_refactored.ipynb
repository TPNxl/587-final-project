{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 82722\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "from dataset_classes.DEAM_CQT_sliding import DEAM_CQT_Dataset_Sliding\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "annot_path = \"deam_dataset/DEAM_Annotations/annotations/annotations averaged per song/dynamic (per second annotations)/arousal.csv\"\n",
    "audio_path = \"deam_dataset/DEAM_audio/MEMD_audio/\"\n",
    "transform_path = \"transforms/\"\n",
    "transform_name = \"testing\"\n",
    "train_dataset = DEAM_CQT_Dataset_Sliding(annot_path=annot_path, audio_path=audio_path, save_files=True, transform_path=transform_path, transform_name=transform_name, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "print(\"Dataset length:\", train_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2000\n",
      "tensor([[0.1883, 0.0452, 0.0435, 0.0295, 0.0148, 0.0241, 0.0337, 0.0474, 0.0216,\n",
      "         0.0205, 0.0857, 1.0000],\n",
      "        [0.4127, 1.0000, 0.6858, 0.1487, 0.0606, 0.1031, 0.0775, 0.8512, 0.2708,\n",
      "         0.4285, 0.6484, 0.4822],\n",
      "        [1.0000, 0.4995, 0.3481, 0.1437, 0.1892, 0.3886, 0.5801, 0.5625, 0.2847,\n",
      "         0.3010, 0.9267, 0.9013],\n",
      "        [0.0447, 0.4822, 1.0000, 0.0689, 0.0379, 0.0305, 0.0224, 0.1932, 0.0576,\n",
      "         0.0935, 0.0745, 0.0910],\n",
      "        [1.0000, 0.1667, 0.0776, 0.0318, 0.0346, 0.1408, 0.0723, 0.0797, 0.0521,\n",
      "         0.0769, 0.1247, 0.3175],\n",
      "        [1.0000, 0.1068, 0.0700, 0.0595, 0.0437, 0.0362, 0.0544, 0.2939, 0.1004,\n",
      "         0.0776, 0.0678, 0.1546],\n",
      "        [0.2394, 0.0867, 0.0338, 0.0312, 0.1079, 0.2550, 0.0467, 0.1268, 0.2296,\n",
      "         1.0000, 0.7492, 0.2022],\n",
      "        [1.0000, 0.0596, 0.0332, 0.0179, 0.0277, 0.1727, 0.0203, 0.0983, 0.0180,\n",
      "         0.0214, 0.0543, 0.1418],\n",
      "        [1.0000, 0.2148, 0.0459, 0.0277, 0.0400, 0.2163, 0.0632, 0.0810, 0.0228,\n",
      "         0.0343, 0.1556, 0.3079],\n",
      "        [0.0680, 0.6334, 1.0000, 0.0685, 0.0267, 0.0471, 0.0321, 0.8886, 0.0551,\n",
      "         0.0838, 0.3202, 0.1751]], dtype=torch.float64)\n",
      "\n",
      "-----\n",
      "\n",
      "tensor([-0.0927, -0.0836, -0.0876, -0.0834, -0.0834, -0.0774, -0.0843, -0.0817,\n",
      "        -0.0790, -0.0770], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "index = 2000\n",
    "print(\"Index: \", index)\n",
    "(data, target) = train_dataset.__getitem__(index)\n",
    "print(data[0:10])\n",
    "print()\n",
    "print(\"-----\")\n",
    "print()\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1622\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.internal_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2v_cqt_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
