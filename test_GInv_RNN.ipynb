{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 19464\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "from dataset_classes.DEAM_CQT_circshift import DEAM_CQT_Dataset_With_CircShift\n",
    "from models.GInv_RNN import *\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "annot_path = \"deam_dataset/DEAM_Annotations/annotations/annotations averaged per song/dynamic (per second annotations)/arousal.csv\"\n",
    "audio_path = \"deam_dataset/DEAM_audio/MEMD_audio/\"\n",
    "transform_path = \"transforms/\"\n",
    "transform_name = \"testing\"\n",
    "train_dataset = DEAM_CQT_Dataset_With_CircShift(annot_path=annot_path, audio_path=audio_path, save_files=True, transform_path=transform_path, transform_name=transform_name, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "print(\"Dataset length:\", train_dataset.__len__())\n",
    "\n",
    "model = GInvariantRNN_Model(12, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  6490\n",
      "tensor([[0.2844, 0.2593, 0.4359, 0.6561, 0.3873, 0.6633, 1.0000, 0.9221, 0.5243,\n",
      "         0.2822, 0.2675, 0.2227],\n",
      "        [0.4498, 0.9124, 0.9946, 1.0000, 0.5334, 0.6690, 0.7309, 0.5835, 0.7154,\n",
      "         0.9832, 0.8229, 0.4940],\n",
      "        [0.7028, 0.6401, 0.4414, 0.6759, 0.8171, 0.5889, 0.4380, 0.5456, 0.8469,\n",
      "         0.5751, 1.0000, 0.7589],\n",
      "        [0.1555, 0.2468, 0.1442, 0.1374, 0.2166, 0.6660, 0.1493, 0.1069, 0.3425,\n",
      "         0.8385, 1.0000, 0.2993],\n",
      "        [0.6365, 0.9797, 0.8712, 0.7587, 0.6342, 0.7493, 0.5978, 0.5473, 0.8447,\n",
      "         1.0000, 0.5186, 0.7038],\n",
      "        [0.5994, 0.6616, 0.7110, 1.0000, 0.4828, 0.7185, 0.8409, 0.2850, 0.4653,\n",
      "         0.4785, 0.9594, 0.5794],\n",
      "        [0.4024, 1.0000, 0.7500, 0.5226, 0.4274, 0.2588, 0.5076, 0.2549, 0.2222,\n",
      "         0.3056, 0.2578, 0.4979],\n",
      "        [0.2989, 0.5274, 0.2739, 0.3743, 0.1686, 0.2095, 0.3006, 0.2312, 0.3099,\n",
      "         0.3119, 0.3766, 1.0000],\n",
      "        [0.4857, 0.7207, 0.9902, 0.6009, 0.4052, 0.6504, 0.8244, 0.7280, 0.6983,\n",
      "         0.9999, 0.9364, 1.0000],\n",
      "        [1.0000, 0.2429, 0.1959, 0.1959, 0.1236, 0.1269, 0.1121, 0.1150, 0.1199,\n",
      "         0.1340, 0.1624, 0.4266]], dtype=torch.float64)\n",
      "\n",
      "-----\n",
      "\n",
      "tensor([-0.2448, -0.2380, -0.2379, -0.2452, -0.2461, -0.2466, -0.2353, -0.2309,\n",
      "        -0.2272, -0.2326, -0.2122, -0.1951, -0.1949, -0.1953, -0.1945, -0.2132,\n",
      "        -0.2433, -0.2459, -0.2438, -0.2506, -0.2376, -0.2418, -0.2435, -0.2573,\n",
      "        -0.2597, -0.2567, -0.2559, -0.2581, -0.2593, -0.2545, -0.2581, -0.2608,\n",
      "        -0.2635, -0.2493, -0.2400, -0.2473, -0.2519, -0.2431, -0.2329, -0.2306,\n",
      "        -0.2290, -0.2303, -0.2292, -0.2292, -0.2292, -0.2292, -0.2292, -0.2292,\n",
      "        -0.2292, -0.2320, -0.2336, -0.2362, -0.2381, -0.2600, -0.2643, -0.2601,\n",
      "        -0.2506, -0.2525, -0.2485, -0.2472], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "shift = 4\n",
    "item = 2\n",
    "index = item + train_dataset.internal_size() * shift\n",
    "print(\"Index: \", index)\n",
    "(data, target) = train_dataset.__getitem__(index)\n",
    "print(data[0:10])\n",
    "print()\n",
    "print(\"-----\")\n",
    "print()\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1, 30])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(data)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2v_cqt_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
